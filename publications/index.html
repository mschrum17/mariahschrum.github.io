<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">

    <title>Mariah Schrum | Publications</title>
    <meta name="description" content="A beautiful Jekyll theme for academics">

    <!-- Fonts and Icons -->
    <link rel="stylesheet" type="text/css"
          href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"/>

    <!-- CSS Files -->
    <link rel="stylesheet" href="/assets/css/all.min.css">
    <link rel="stylesheet" href="/assets/css/academicons.min.css">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="/publications/">
</head>
<body>
<!-- Header -->
<nav id="navbar" class="navbar fixed-top navbar-expand-md grey lighten-5 z-depth-1 navbar-light">
    <div class="container-fluid p-0">

        <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Mariah</span>
            Schrum</a>
        <div class="row ml-1 ml-sm-0">
          <span class="contact-icon text-center" style="line-height: 1em;">
             <a href="mailto:mschrum3@gatech.edu"><i class="fa fa-envelope-square gm-icon"></i></a>
            <a href="https://scholar.google.com/citations?user=QuzrQzIAAAAJ&hl=en" target="_blank"
               title="Google Scholar"><i class="ai ai-google-scholar-square gs-icon"></i></a>
            <a href="https://github.com/mschrum17" target="_blank" title="GitHub"><i
                    class="fab fa-github-square gh-icon"></i></a>
            <a href="https://www.linkedin.com/in/mariah-schrum-531972156/" target="_blank" title="LinkedIn"><i
                    class="fab fa-linkedin li-icon"></i></a>
            <a href="https://twitter.com/mariah17schrum" target="_blank" title="Twitter"><i
                    class="fab fa-twitter-square tw-icon"></i></a>
          </span>
        </div>

        <button class="navbar-toggler ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="navbar-toggler-icon"></span>
        </button>
        <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">
                <li class="nav-item ">
                    <a class="nav-link" href="/">
                        About

                    </a>
                </li>

                <li class="nav-item ">
                    <a class="nav-link" href="/assets/pdf/vitae.pdf">
                        Curriculum Vitae
                    </a>
                </li>


                <!--<li class="nav-item ">
                    <a class="nav-link" href="/projects/">
                        Projects

                    </a>
                </li>-->



                <li class="nav-item navbar-active font-weight-bold">
                    <a class="nav-link" href="/publications/">
                        Publications

                        <span class="sr-only">(current)</span>

                    </a>
                </li>

                <!--              <li class="nav-item ">-->
                <!--                  <a class="nav-link" href="/teaching/">-->
                <!--                    Teaching-->
                <!--                    -->
                <!--                  </a>-->
                <!--              </li>-->

                <!--              <li class="nav-item ">-->
                <!--                  <a class="nav-link" href="/service/">-->
                <!--                    Service-->
                <!--                                        -->
                <!--                  </a>-->
                <!--              </li>-->

            </ul>
        </div>
    </div>
</nav>

<!-- Scrolling Progress Bar -->
<progress id="progress" value="0">
    <div class="progress-container">
        <span class="progress-bar"></span>
    </div>
</progress>

<!-- Content -->
<div class="content">

    <h1>Publications</h1>
    <h6>
        <nobr><em>*</em></nobr>
        denotes equal contribution and joint lead authorship.
    </h6>
    <h6>Blue - Conference Papers.</h6>
    <h6>Red - Workshop and Doctoral Consortia Papers.</h6>
    <h6>Orange - Journal Papers.</h6>


    <p><br/></p>


    <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="/assets/pdf/mixed_initiative.pdf" target="_blank">
                                Neurips
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="zaidi2022WTR" class="col p-0">
                                <h5 class="title mb-0">Mixed-Initiative Multiagent Apprenticeship Learning for Human Training of Robot Teams</h5>
                                <div class="author">

                                    <nobr>Esi Seraj</nobr>
                                    <nobr>Jerry Yuyang Xiong,</nobr>
                                    <nobr><em>Mariah Schrum</em>,</nobr>



                                    and

                                    <nobr>Matthew Gombolay,</nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Neural Information Processing
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#zaidi2023WTR-abstract" role="button" aria-expanded="false"
                                       aria-controls="zaidi2023WTR-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://openreview.net/pdf?id=VCOZaczCHg"
                                       target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="zaidi2023WTR-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            —Extending recent advances in Learning from Demonstration (LfD) frameworks to multi-robot settings poses critical challenges such as environment non-stationarity due to partial observability which is detrimental to the applicability of existing methods. Although prior work has shown that enabling communication among agents of a robot team can alleviate such issues, creating inter-agent communication under existing Multi-Agent LfD (MA-LfD) frameworks requires the human expert to provide demonstrations for both environment actions and communication actions, which necessitates an efficient communication strategy on a known message spaces. To address this problem, we propose Mixed-Initiative Multi-Agent Apprenticeship Learning (MixTURE). MixTURE enables robot teams to learn from a human expert-generated data a preferred policy to accomplish a collaborative task, while simultaneously learning emergent inter-agent communication to enhance team coordination. The key ingredient to MixTURE's success is automatically learning a communication policy, enhanced by a mutual-information maximizing reverse model that rationalizes the underlying expert demonstrations without the need for human generated data or an auxiliary reward function. MixTURE outperforms a variety of relevant baselines on diverse data generated by human experts in complex heterogeneous domains. MixTURE is the first MA-LfD framework to enable learning multi-robot collaborative policies directly from real human data, resulting in ~44% less human workload, and ~46% higher usability score.

                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>

                </li>

    <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold orange darken-1 align-middle" style="width: 65px;"
                               href="/assets/pdf/MAVERIC_TRO-16.pdf" target="_blank">
                               T-RO
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="zaidi2022WTR" class="col p-0">
                                <h5 class="title mb-0">MAVERIC: A Data-Driven Approach to Personalized Autonomous Driving</h5>
                                <div class="author">

                                    <nobr><em>Mariah Schrum</em>,</nobr>
                                    <nobr>Emily Sumner,</nobr>
                                    <nobr>Matthew Gombolay,</nobr>



                                    and

                                    <nobr>Andrew Best,</nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        IEEE Transactions on Robotics
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#zaidi2023WTR-abstract" role="button" aria-expanded="false"
                                       aria-controls="zaidi2023WTR-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10415518"
                                       target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="zaidi2023WTR-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            — Personalization of autonomous vehicles (AV) may significantly increase trust, use, and acceptance. In particular,
                                            we hypothesize that the similarity of an AV’s driving style
                                            compared to the end-user’s driving style will have a major
                                            impact on end-user’s willingness to use the AV. To investigate
                                            the impact of driving style on user acceptance, we 1) develop
                                            a data-driven approach to personalize driving style and 2)
                                            demonstrate that personalization significantly impacts attitudes
                                            towards AVs. Our approach learns a high-level model that tunes
                                            low-level controllers to ensure safe and personalized control of
                                            the AV. The key to our approach is learning an informative,
                                            personalized embedding that represents a user’s driving style.
                                            Our framework is capable of calibrating the level of aggression
                                            so as to optimize driving style based upon driver preference.
                                            Across two human subject studies (n = 54), we first demonstrate
                                            our approach mimics the driving styles of end-users and can
                                            tune attributes of style (e.g., aggressiveness). Second, we
                                            investigate the factors (e.g., trust, personality etc.) that impact
                                            homophily, i.e. an individual’s preference for a driving style
                                            similar to their own. We find that our approach generates
                                            driving styles consistent with end-user styles (p < .001) and
                                            participants rate our approach as more similar to their level of
                                            aggressiveness (p = .002). We find that personality (p < .001),
                                            perceived similarity (p < .001), and high-velocity driving style
                                            (p = .0031) significantly modulate the effect of homophily.

                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>

                </li>

    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2023</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://sites.google.com/view/concatenate-hri/home?pli=1" target="_blank">
                                HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="zaidi2022WTR" class="col p-0">
                                <h5 class="title mb-0">Privacy and Personalization: Transparency, Acceptance, and the
Ethics of Personalized Robots
                                </h5>
                                <div class="author">

                                    <nobr><em>Mariah Schrum</em></nobr>

                                    and

                                    <nobr>Matthew Gombolay,</nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Human-Robot Interaction Social Robots Personalisation Workshop
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#zaidi2023WTR-abstract" role="button" aria-expanded="false"
                                       aria-controls="zaidi2023WTR-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/Ethics_of_Personalization.pdf"
                                       target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="zaidi2023WTR-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            — To effectively support humans, machines must be capable of recognizing individual desires, abilities, and characteristics and adapt
to account for differences across individuals. However, personalization does not come without a cost. In many domains, for robots
to effectively personalize their behavior, the robot must solicit often private and intimate information about an end-user so as to
optimize the interaction. However, not all end-users may be comfortable sharing this information, especially if the end-user is not
provided with insight into why the robot is requesting it. As HRI
researchers, we have the responsibility of ensuring the robots we
create do not infringe upon the privacy rights of end-users and that
end-users are provided with the means to make informed decisions
about the information they share with robots. While prior work
has investigated willingness to share information in the context
of consumerism, no prior work has investigated the impact of domain, type of requested information, or explanations on end-user’s
comfort and acceptance of a personalized robot. To gain a better understanding of these questions, we propose an experimental design
in which we investigate the impact of domain, nature of personal
information requested, and the role of explanations on robot transparency and end-user willingness to share information. Our goal
of this study is to provide guidance for HRI researchers who are
conducting work in personalization by examining the factors that
may impact transparency and acceptance of personalized robots

                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>

                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://humanrobotinteraction.org/2023/" target="_blank">
                                HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="krishna2023HRI" class="col p-0">
                                <h5 class="title mb-0">The Effect of Robot Skill Level and Communication in Rapid,
                                    Proximate Human-Robot Collaboration</h5>
                                <div class="author">

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Kin Man Lee*</a>,</nobr>

                                    <nobr><a href="https://www.linkedin.com/in/arjun-kris" target="_blank">Arjun
                                        Krishna*</a>,
                                    </nobr>

                                    <nobr><a href="https://core-robotics.gatech.edu/people/zulfiqar-zaidi/"
                                             target="_blank">Zulfiqar Zaidi</a>,
                                    </nobr>

                                    <nobr>Rohan Paleja,</nobr>

                                    <nobr><a href="http://www.letianchen.me/" target="_blank">Letian Chen</a>,</nobr>


                                    <nobr><a href="https://www.linkedin.com/in/erin-hedlund-botti" target="_blank">Erin
                                        Hedlund-Botti</a>,
                                    </nobr>

                                    <nobr><em>Mariah Schrum</em>,</nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        ACM/IEEE International Conference on Human-Robot Interaction (HRI), 2023
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#krishna2023HRI-abstract" role="button" aria-expanded="false"
                                       aria-controls="krishna2023HRI-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/krishna_VAIN.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/CORE-Robotics-Lab/ICCT" target="_blank">Code</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="krishna2023HRI-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            As high-speed, agile robots become more commonplace, these robots will have
                                            the potential to better aid and collaborate with humans. However, due to the
                                            increased agility and functionality of these robots, close collaboration
                                            with humans can create safety concerns that alter team dynamics and degrade
                                            task performance. In this work, we aim to enable the deployment of safe and
                                            trustworthy agile robots that operate in proximity with humans. We do so by
                                            1) Proposing a novel human-robot doubles table tennis scenario to serve as a
                                            testbed for studying agile, proximate human-robot collaboration and 2)
                                            Conducting a user-study to understand how attributes of the robot (e.g.,
                                            robot competency or capacity to communicate) impact team dynamics, perceived
                                            safety, and perceived trust, and how these latent factors affect human-robot
                                            collaboration (HRC) performance. We find that robot competency significantly
                                            increases perceived trust ($p<.001$), extending skill-to-trust assessments
                                            in prior studies to agile, proximate HRC. Furthermore, interestingly, we
                                            find that when the robot vocalizes its intention to perform a task, it
                                            results in a significant decrease in team performance (p=.037) and perceived
                                            safety of the system (p=.009).
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>
            </ol>

            </ol>
        </div>
    </div>

    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2022</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold orange darken-1 align-middle" style="width: 65px;"
                               href="https://www.tandfonline.com/toc/hihc20/current" target="_blank">
                                IJHCI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="chen2022CoRL" class="col p-0">
                                <h5 class="title mb-0">Explainable Artificial Intelligence: Evaluating the Objective and Subjective Impacts of xAI on Human-Agent Interaction</h5>
                                <div class="author">

                                    <nobr>Andrew Silva,</nobr>

                                    <nobr><em>Mariah Schrum</em>,</nobr>

                                    <nobr>Erin Hedlund-Botti,</nobr>

                                    <nobr>Nakul Gopalan,</nobr>



                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>



                                <div>
                                    <p class="periodical font-italic">
                                        International Journal on Human-Computer Interaction, 2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#chen2022CoRL-abstract" role="button" aria-expanded="false"
                                       aria-controls="chen2022CoRL-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/xai.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/CORE-Robotics-Lab/ICCT" target="_blank">Code</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="chen2022CoRL-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                           Intelligent agents must be able to communicate intentions and explain their decision-making processes to build trust, foster confidence, and improve human-agent team dynamics. Recognizing this need, academia and industry are rapidly proposing new ideas, methods, and frameworks to aid in the design of more explainable AI. Yet, there remains no standardized metric or experimental protocol for benchmarking new methods, leaving researchers to rely on their own intuition or ad hoc methods for assessing new concepts. In this work, we present the first comprehensive (n = 286) user study testing a wide range of approaches for explainable machine learning, including feature importance, probability scores, decision trees, counterfactual reasoning, natural language explanations, and case-based reasoning, as well as a baseline condition with no explanations. We provide the first large-scale empirical evidence of the effects of explainability on human-agent teaming. Our results will help to guide the future of explainability research by highlighting the benefits of counterfactual explanations and the shortcomings of confidence scores for explainability. We also propose a novel questionnaire to measure explainability with human participants, inspired by relevant prior work and correlated with human-agent teaming metrics.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                    </div>
                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold orange darken-1 align-middle" style="width: 65px;"
                               href="https://dl.acm.org/journal/thri" target="_blank">
                                THRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="chen2022CoRL" class="col p-0">
                                <h5 class="title mb-0">Concerning Trends in Likert Scale Usage in Human-Robot Interaction: Towards Improving Best Practices</h5>
                                <div class="author">


                                    <nobr><em>Mariah Schrum</em>*,</nobr>
                                    <nobr>Leng Ghuy*,</nobr>

                                    <nobr>Erin Hedlund-Botti,</nobr>

                                    <nobr>Manisha Natarajan,</nobr>

                                    <nobr>Michael Johnson,</nobr>



                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>



                                <div>
                                    <p class="periodical font-italic">
                                        Transactions on Human-Robot Interaction, 2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#chen2022CoRL-abstract" role="button" aria-expanded="false"
                                       aria-controls="chen2022CoRL-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/thri.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/CORE-Robotics-Lab/ICCT" target="_blank">Code</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="chen2022CoRL-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                           As robots become more prevalent, the importance of the ield of human-robot interaction (HRI) grows
accordingly. As such, we should endeavor to employ the best statistical practices in HRI research. Likert
scales are commonly used metrics in HRI to measure perceptions and attitudes. Due to misinformation or
honest mistakes, many HRI researchers do not adopt best practices when analyzing Likert data. We conduct
a review of psychometric literature to determine the current standard for Likert scale design and analysis.
Next, we conduct a survey of ive years of the International Conference on Human-Robot Interaction (HRIc)
(2016 through 2020) and report on incorrect statistical practices and design of Likert scales [1 ś 3, 5 , 7]. During
these years, only 4 of the 144 papers applied proper statistical testing to correctly-designed Likert scales. We
additionally conduct a survey of best practices across several venues and provide a comparative analysis to
determine how Likert practices difer across the ield of Human-Robot Interaction. We ind that a venue’s
impact score negatively correlates with number of Likert related errors and acceptance rate, and total number of
papers accepted per venue positively correlates with the number of errors. We also ind statistically signiicant
diferences between venues for the frequency of misnomer and design errors. Our analysis suggests there
are areas for meaningful improvement in the design and testing of Likert scales. Based on our indings, we
provide guidelines and a tutorial for researchers for developing and analyzing Likert scales and associated
data. We also detail a list of recommendations to improve the accuracy of conclusions drawn from Likert data
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold orange darken-1 align-middle" style="width: 65px;"
                               href="https://www.ieee-ras.org/publications/ra-l" target="_blank">
                                IEEE RA-L
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="krishna2022Agility" class="col p-0">
                                <h5 class="title mb-0">Meta-Active Learning in Probabilistically Safe Optimization</h5>
                                <div class="author">

                                    <nobr><em>Mariah Schrum</em>,</nobr>


                                    <nobr>Mark Connolly,</nobr>

                                    <nobr>Eric Cole,</nobr>

                                    <nobr>Mihir Ghetiya,</nobr>

                                    <nobr>Robert Gross,</nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        International Conference on Intelligent Robots and Systems (IROS), 2022
                                    </p>
                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        IEEE Robotics and Automation Letters, 2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#krishna2022Agility-abstract" role="button" aria-expanded="false"
                                       aria-controls="krishna2022Agility-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/safe_metal.pdf"
                                       target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="krishna2022Agility-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                           When a robotic system is faced with uncertainty, the system must take calculated risks to gain information as efficiently as possible while ensuring system safety. The need to safely and efficiently gain information in the face of uncertainty spans domains from healthcare to search and rescue. To efficiently learn when data is scarce or difficult to label, active learning acquisition functions intelligently select a data point that, if the label were known, would most improve the estimate of the unknown model. Unfortunately, prior work in active learning suffers from an inability to accurately quantify information-gain, generalize to new domains, and ensure safe operation. To overcome these limitations, we develop Safe MetAL, a probabilistically-safe, active learning algorithm which meta-learns an acquisition function for selecting sample efficient data points in safety critical domains. The key to our approach is a novel integration of meta-active learning and chance-constrained optimization. We (1) meta-learn an acquisition function based on sample history, (2) encode this acquisition function in a chance-constrained optimization framework, and (3) solve for an information-rich set of data points while enforcing probabilistic safety guarantees. We present state-of-the-art results in active learning of the model of a damaged UAV and in learning the optimal parameters for deep brain stimulation. Our approach achieves a 41% improvement in learning the optimal model and a 20% speedup in computation time compared to active and meta-learning approaches while ensuring safety of the system.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>

                </li>

                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://corl2022.org/" target="_blank">
                                CoRL
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2022RSS" class="col p-0">
                                <h5 class="title mb-0">Reciprocal MIND MELD: Improving Learning From Demonstration via Personalized, Reciprocal Teaching</h5>
                                <div class="author">


                                    <nobr><em>Mariah Schrum</em>,</nobr>

                                    <nobr>Erin Hedlund-Botti</nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Robot Learning, 2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2022RSS-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2022RSS-abstract">Abstract</a>
                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/rmm.pdf" target="_blank">PDF</a>

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="paleja2022RSS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            Endowing robots with the ability to learn novel tasks via demonstrations will increase the accessibility of robots for non-expert, non-roboticists. However, research has shown that humans can be poor teachers, making it difficult for robots to effectively learn from humans. If the robot could instruct humans how to provide better demonstrations, then humans might be able to effectively teach a broader range of novel, out-of-distribution tasks. In this work, we introduce Reciprocal MIND MELD, a framework in which the robot learns the way in which a demonstrator is suboptimal and utilizes this information to provide feedback to the demonstrator to improve upon their demonstrations. We additionally develop an Embedding Predictor Network which learns to predict the demonstrator’s suboptimality online without the need for optimal labels. In a series of human-subject experiments in a driving simulator domain, we demonstrate that robotic feedback can effectively improve human demonstrations in two dimensions of suboptimality (p < .001) and that robotic feedback translates into better learning outcomes for a robotic agent on novel tasks (p = .045).
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://humanrobotinteraction.org/2022/" target="_blank">
                                HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="pimentel2022Scaling" class="col p-0">
                                <h5 class="title mb-0">Mind meld: Personalized meta-learning for robot-centric imitation learning</h5>
                                <div class="author">
                                    <nobr><em>Mariah Schrum*</em>,</nobr>

                                    <nobr>Erin Hedlund-Botti*</nobr>


                                    <nobr>Nina Moorman,
                                    </nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Robot Learning (CoRL), 2021
                                        <br>
                                        <nobr><em style="color:orange;">Best Technical Paper</em></nobr>
                                    </p>
                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Human-Robot Interaction, 2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#pimentel2022Scaling-abstract" role="button" aria-expanded="false"
                                       aria-controls="pimentel2022Scaling-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/mindmeld.pdf" target="_blank">PDF</a>

                                </div>


                                <div class="col mt-2 p-0">
                                    <div id="pimentel2022Scaling-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            —Learning from demonstration (LfD) techniques seek
to enable users without computer programming experience to
teach robots novel tasks. There are generally two types of
LfD: human- and robot-centric. While human-centric learning
is intuitive, human centric learning suffers from performance
degradation due to covariate shift. Robot-centric approaches,
such as Dataset Aggregation (DAgger), address covariate shift but
can struggle to learn from suboptimal human teachers. To create
a more human-aware version of robot-centric LfD, we present
Mutual Information-driven Meta-learning from Demonstration
(MIND MELD). MIND MELD meta-learns a mapping from
suboptimal and heterogeneous human feedback to optimal labels,
thereby improving the learning signal for robot-centric LfD. The
key to our approach is learning an informative personalized embedding using mutual information maximization via variational
inference. The embedding then informs a mapping from human
provided labels to optimal labels. We evaluate our framework in
a human-subjects experiment, demonstrating that our approach
improves corrective labels provided by human demonstrators.
Our framework outperforms baselines in terms of ability to reach
the goal (p < .001), average distance from the goal (p = .006),
and various subjective ratings (p = .008).

                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>

                </li>



                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://hripioneers.org/archives/hri22/" target="_blank">
                                Pioneers
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2022AAMAS" class="col p-0">
                                <h5 class="title mb-0">Personalized meta-learning for domain agnostic learning from demonstration </h5>
                                <div class="author">

                                    <nobr><em>Mariah Schrum,</em>
                                    </nobr>

                                    <nobr>Erin Hedlund-Botti,
                                    </nobr>



                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        HRI Pioneers Doctoral Consortium,
                                        2022
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2022AAMAS-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2022AAMAS-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/hri_pioneers.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/nvidia.com/active-concept-learning" target="_blank">Project Website</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="paleja2022AAMAS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            For robots to perform novel tasks in the real-world,
they must be capable of learning from heterogeneous, non-expert
human teachers across various domains. Yet, novice human
teachers often provide suboptimal demonstrations, making it
difficult for robots to successfully learn. Therefore, to effectively
learn from humans, we must develop learning methods that can
account for teacher suboptimality and can do so across various
robotic platforms. To this end, we introduce Mutual Information
Driven Meta-Learning from Demonstration (MIND MELD) [12,
13], a personalized meta-learning framework which meta-learns
a mapping from suboptimal human feedback to feedback closer
to optimal, conditioned on a learned personalized embedding.
In a human subjects study, we demonstrate MIND MELD’s
ability to improve upon suboptimal demonstrations and learn
meaningful, personalized embeddings. We then propose Domain
Agnostic MIND MELD, which learns to transfer the personalized
embedding learned in one domain to a novel domain, thereby al-
lowing robots to learn from suboptimal humans across disparate
platforms (e.g., self-driving car or in-home robot).
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                </li>



            </ol>
        </div>
    </div>


    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2021</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">



                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold pink darken-1 align-middle" style="width: 65px;"
                               href="https://ai-hri.github.io/2022/" target="_blank">
                                AI-HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="Dias2021" class="col p-0">
                                <h5 class="title mb-0">Improving robot-centric learning from demonstration via personalized embeddings</h5>
                                <div class="author">

                                    <nobr><em>Mariah Schrum,</em>
                                    </nobr>

                                    <nobr>Erin Hedlund-Botti*,</nobr>
                                                          and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Artificial Intelligence for Human-Robot Interaction Symposium, 2021
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#Dias2021-abstract" role="button" aria-expanded="false"
                                       aria-controls="Dias2021-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/ai-hri.pdf"
                                       target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/A4k3B3uewBs?t=8337" target="_blank">Talk</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="Dias2021-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">

Learning from demonstration (LfD) techniques seek to en-
able novice users to teach robots novel tasks in the real world.
However, prior work has shown that robot-centric LfD ap-
proaches, such as Dataset Aggregation (DAgger), do not per-
form well with human teachers. DAgger requires a human
demonstrator to provide corrective feedback to the learner ei-
ther in real-time, which can result in degraded performance
due to suboptimal human labels, or in a post hoc manner
which is time intensive and often not feasible. To address
this problem, we present Mutual Information-driven Meta-
learning from Demonstration (MIND MELD), which meta-
learns a mapping from poor quality human labels to pre-
dicted ground truth labels, thereby improving upon the per-
formance of prior LfD approaches for DAgger-based train-
ing. The key to our approach for improving upon suboptimal
feedback is mutual information maximization via variational
inference. Our approach learns a meaningful, personalized
embedding via variational inference which informs the map-
ping from human provided labels to predicted ground truth
labels. We demonstrate our framework in a synthetic domain
and in a human-subjects experiment, illustrating that our ap-
proach improves upon the corrective labels provided by a hu-
man demonstrator by 63%.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://www.tandfonline.com/journals/tciv20" target="_blank">
                                alt-HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="Dias2021" class="col p-0">
                                <h5 class="title mb-0">Four years in review: Statistical practices of likert scales in human-robot interaction studies</h5>
                                <div class="author">

                                    <nobr><em>Mariah Schrum,</em>
                                    </nobr>

                                    <nobr>Leng Ghuy*,</nobr>
                                    <nobr>Michael Johnson*,</nobr>


                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Human-Robot Interaction, 2021
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#Dias2021-abstract" role="button" aria-expanded="false"
                                       aria-controls="Dias2021-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/fouryears.pdf"
                                       target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://youtu.be/A4k3B3uewBs?t=8337" target="_blank">Talk</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="Dias2021-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            As robots become more prevalent, the importance of the field of human-robot interaction (HRI) grows accordingly. As such, we should endeavor to employ the best statistical practices. Likert scales are commonly used metrics in HRI to measure perceptions and attitudes. Due to misinformation or honest mistakes, most HRI researchers do not adopt best practices when analyzing Likert data. We conduct a review of psychometric literature to determine the current standard for Likert scale design and analysis. Next, we conduct a survey of four years of the International Conference on Human-Robot Interaction (2016 through 2019) and report on incorrect statistical practices and design of Likert scales. During these years, only 3 of the 110 papers applied proper statistical testing to correctly-designed Likert scales. Our analysis suggests there are areas for meaningful improvement in the design and testing of Likert scales. Lastly, we provide recommendations to improve the accuracy of conclusions drawn from Likert data.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>


                 <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold light-blue darken-1 align-middle" style="width: 65px;"
                               href="https://humanrobotinteraction.org/2021/" target="_blank">
                                HRI
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="paleja2022AAAI" class="col p-0">
                                <h5 class="title mb-0">Effects of Social Factors and Team Dynamics on Adoption of
Collaborative Robot Autonomy</h5>
                                <div class="author">

                                    <nobr><em>Mariah Schrum*</em>,</nobr>
                                    <nobr>Glen Neville*,</nobr>
                                    <nobr>Michael Johnson*,</nobr>
                                    <nobr>Nina Moorman,</nobr>
                                    <nobr>Karen Feigh,</nobr>

                                    and

                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        Conference on Human-Robot Interaction, 2021
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#paleja2022AAAI-abstract" role="button" aria-expanded="false"
                                       aria-controls="paleja2022AAAI-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/hri21.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://sites.google.com/nvidia.com/active-concept-learning" target="_blank">Project Website</a>-->

                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="paleja2022AAAI-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                            As automation becomes more prevalent, the fear of job loss due
to automation increases [ 22 ]. Workers may not be amenable to
working with a robotic co-worker due to a negative perception
of the technology. The attitudes of workers towards automation
are influenced by a variety of complex and multi-faceted factors
such as intention to use, perceived usefulness and other external
variables [15 ]. In an analog manufacturing environment, we explore
how these various factors influence an individual’s willingness
to work with a robot over a human co-worker in a collaborative
Lego building task. We specifically explore how this willingness is
affected by: 1) the level of social rapport established between the
individual and his or her human co-worker, 2) the anthropomorphic
qualities of the robot, and 3) factors including trust, fluency and
personality traits. Our results show that a participant’s willingness
to work with automation decreased due to lower perceived team
fluency (p=0.045), rapport established between a participant and
their co-worker (p=0.003), the gender of the participant being male
(p=0.041), and a higher inherent trust in people (p=0.018).
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>



            </ol>


            </ol>
        </div>
    </div>

    <div class="row m-0 p-0" style="border-top: 1px solid #ddd; flex-direction: row-reverse;">
        <div class="col-sm-1 mt-2 p-0 pr-1">
            <h3 class="bibliography-year">2020</h3>
        </div>
        <div class="col-sm-11 p-0">
            <ol class="bibliography">
                   <li>
                    <div class="row m-0 mt-3 p-0">
                        <div class="col-sm-1 p-0 abbr">
                            <a class="badge font-weight-bold orange darken-1 align-middle" style="width: 65px;"
                               href="https://www.ieee-ras.org/publications/ra-l" target="_blank">
                                RA-L
                            </a>
                        </div>
                        <div class="col-sm-11 mt-2 mt-sm-0 p-0 pl-xs-0 pl-sm-4 pr-xs-0 pr-sm-2">

                            <div id="Paleja2021NeurIPS" class="col p-0">
                                <h5 class="title mb-0">When your robot breaks: Active learning during plant failure</h5>
                                <div class="author">

                                    <nobr><em>Mariah Schrum</em>,</nobr>


                                    and
                                    <nobr><a href="https://core-robotics.gatech.edu/people/matthew-gombolay/"
                                             target="_blank">Matthew Gombolay</a>.
                                    </nobr>

                                </div>

                                <div>
                                    <p class="periodical font-italic">
                                        International Conference on Robotics and Automation (ICRA), 2020
                                    </p>
                                </div>
                                <div>
                                    <p class="periodical font-italic">
                                        IEEE Robotics and Automation Letters (RA-L), 2020
                                    </p>
                                </div>

                                <div class="col p-0">

                                    <a class="badge grey waves-effect font-weight-light mr-1" data-toggle="collapse"
                                       href="#Paleja2021NeurIPS-abstract" role="button" aria-expanded="false"
                                       aria-controls="Paleja2021NeurIPS-abstract">Abstract</a>

                                    <a class="badge grey waves-effect font-weight-light mr-1"
                                       href="/assets/pdf/icra20.pdf" target="_blank">PDF</a>
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://arjunsripathy.github.io/model_switching/" target="_blank">Project Website</a>-->
                                    <!--          <a class="badge grey waves-effect font-weight-light mr-1" href="https://github.com/arjunsripathy/model_switching" target="_blank">Code</a>-->


                                </div>

                                <div class="col mt-2 p-0">
                                    <div id="Paleja2021NeurIPS-abstract" class="collapse">
                                        <div class="abstract card card-body font-weight-light mr-0 mr-sm-3 p-3">
                                           Detecting and adapting to catastrophic failures in
robotic systems requires a robot to learn its new dynamics
quickly and safely to best accomplish its goals. To address this
challenging problem, we propose probabilistically-safe, online
learning techniques to infer the altered dynamics of a robot at
the moment a failure (e.g., physical damage) occurs. We combine
model predictive control and active learning within a chance-
constrained optimization framework to safely and efficiently
learn the new plant model of the robot. We leverage a neural net-
work for function approximation in learning the latent dynamics
of the robot under failure conditions. Our framework generalizes
to various damage conditions while being computationally light-
weight to advance real-time deployment. We empirically validate
within a virtual environment that we can regain control of a
severely damaged aircraft in seconds and require only 0.1 seconds
to find safe, information-rich trajectories, outperforming state-
of-the-art approaches.
                                        </div>
                                    </div>
                                </div>

                            </div>
                        </div>
                    </div>
                </li>



            </ol>
        </div>
    </div>
</div>

<!-- Footer -->
<footer>
    &copy; Copyright 2023 Rohan Paleja.


</footer>

<!-- Core JavaScript Files -->
<script src="/assets/js/jquery.min.js" type="text/javascript"></script>
<script src="/assets/js/popper.min.js" type="text/javascript"></script>
<script src="/assets/js/bootstrap.min.js" type="text/javascript"></script>
<script src="/assets/js/mdb.min.js" type="text/javascript"></script>
<script async="" src="https://cdnjs.cloudflare.com/ajax/libs/masonry/4.2.2/masonry.pkgd.min.js"
        integrity="sha384-GNFwBvfVxBkLMJpYMOABq3c+d3KnQxudP/mGPkzpZSTYykLBNsZEnG2D9G/X/+7D"
        crossorigin="anonymous"></script>
<script src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script type="text/javascript" async
        src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML"></script>
<script src="/assets/js/common.js"></script>

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
    $(document).ready(function() {
      var navbarHeight = $('#navbar').outerHeight(true);
      $('body').css({ 'padding-top': navbarHeight });
      $('progress-container').css({ 'padding-top': navbarHeight });
      var progressBar = $('#progress');
      progressBar.css({ 'top': navbarHeight });
      var getMax = function() { return $(document).height() - $(window).height(); }
      var getValue = function() { return $(window).scrollTop(); }   
      // Check if the browser supports the progress element.
      if ('max' in document.createElement('progress')) {
        // Set the 'max' attribute for the first time.
        progressBar.attr({ max: getMax() });
        progressBar.attr({ value: getValue() });
    
        $(document).on('scroll', function() {
          // On scroll only the 'value' attribute needs to be calculated.
          progressBar.attr({ value: getValue() });
        });

        $(window).resize(function() {
          var navbarHeight = $('#navbar').outerHeight(true);
          $('body').css({ 'padding-top': navbarHeight });
          $('progress-container').css({ 'padding-top': navbarHeight });
          progressBar.css({ 'top': navbarHeight });
          // On resize, both the 'max' and 'value' attributes need to be calculated.
          progressBar.attr({ max: getMax(), value: getValue() });
        });
      } else {
        var max = getMax(), value, width;
        var getWidth = function() {
          // Calculate the window width as a percentage.
          value = getValue();
          width = (value/max) * 100;
          width = width + '%';
          return width;
        }
        var setWidth = function() { progressBar.css({ width: getWidth() }); };
        setWidth();
        $(document).on('scroll', setWidth);
        $(window).on('resize', function() {
          // Need to reset the 'max' attribute.
          max = getMax();
          setWidth();
        });
      }
    });





</script>

<!-- Code Syntax Highlighting -->
<link href="https://fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet">
<script src="/assets/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>

<!-- Script Used for Randomizing the Projects Order -->
<!-- <script type="text/javascript">
  $.fn.shuffleChildren = function() {
    $.each(this.get(), function(index, el) {
      var $el = $(el);
      var $find = $el.children();

      $find.sort(function() {
        return 0.5 - Math.random();
      });

      $el.empty();
      $find.appendTo($el);
    });
  };
  $("#projects").shuffleChildren();
</script> -->

<!-- Project Cards Layout -->
<script type="text/javascript">
    var $grid = $('#projects');

    // $grid.masonry({ percentPosition: true });
    // $grid.masonry('layout');

    // Trigger after images load.
    $grid.imagesLoaded().progress(function() {
      $grid.masonry({ percentPosition: true });
      $grid.masonry('layout');
    });





</script>

<!-- Enable Tooltips -->
<script type="text/javascript">
    $(function () {
      $('[data-toggle="tooltip"]').tooltip()
    })





</script>

<!-- Google Analytics -->
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
    (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', '', 'auto');
    ga('send', 'pageview');





</script>
</body>
</html>
